{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "from os.path import join\n",
    "from evaluator import Evaluator, CropOffsetFilter\n",
    "from training_models import Autoencoder, DenoisingAutoencoder, AutoencoderEnsemble\n",
    "from monitor_models import Autocovariance, AugmentationCrossCovariances, RepresentationCrossCovariances\n",
    "from metrics import Identity, Variance, CovarianceWeightedAverage\n",
    "from directory import get_history_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp_test\"\n",
    "TRAIN_DATASETS = [\"instance\", \"stead\"]\n",
    "TEST_DATASETS = [\"instance\", \"stead\"]\n",
    "SPLITS = range(0, 5)\n",
    "NUM_EPOCHS = 20\n",
    "FILTERS = [CropOffsetFilter(3.0, 3.0, 30.0, 100.0)]\n",
    "OUTPUT_FILE_NAME = \"summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric):\n",
    "    evaluator = Evaluator(EXP_NAME, training_model_class, monitoring_model_class, \n",
    "                          train_dataset, test_dataset, FILTERS, range(NUM_EPOCHS), split, method_params, metric)\n",
    "                    \n",
    "    roc_vectors = evaluator.get_roc_vectors()\n",
    "    roc_aucs = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        roc_aucs.append(auc(roc_vectors[epoch][\"tpr\"], roc_vectors[epoch][\"fpr\"]))\n",
    "\n",
    "    return roc_aucs\n",
    "\n",
    "def get_validation_losses(training_model_class, train_dataset, split):\n",
    "    model = training_model_class()\n",
    "    df_history = pd.read_csv(get_history_csv_path(EXP_NAME, model.name, train_dataset, split))\n",
    "    return df_history.loc[:, \"val_loss\"]\n",
    "\n",
    "def model_summary(training_model_class, monitoring_model_class, method_params, metric):\n",
    "    evaluations = []\n",
    "    for train_dataset in TRAIN_DATASETS:\n",
    "        for test_dataset in TEST_DATASETS:\n",
    "            for split in SPLITS:\n",
    "                roc_aucs = calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\n",
    "                val_loss = get_validation_losses(training_model_class, train_dataset, split)\n",
    "                evaluations.append({\"Training dataset\": [train_dataset]*NUM_EPOCHS, \n",
    "                                    \"Testing dataset\": [test_dataset]*NUM_EPOCHS, \n",
    "                                    \"Split\": [split]*NUM_EPOCHS, \n",
    "                                    \"Epoch\": range(NUM_EPOCHS), \n",
    "                                    \"Validation loss\": val_loss, \n",
    "                                    \"ROC-AUC\":roc_aucs})\n",
    "\n",
    "    df = pd.DataFrame(evaluations)\n",
    "\n",
    "    df.loc[:, \"Training model\"] = training_model_class().name\n",
    "    df.loc[:, \"Monitoring model\"] = monitoring_model_class().name\n",
    "\n",
    "    return df\n",
    "\n",
    "def grand_summary():\n",
    "    df_autoencoder_autocovariance = model_summary(Autoencoder, Autocovariance, \n",
    "                                                  {}, \n",
    "                                                  CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_denoisingae_autocovariance = model_summary(DenoisingAutoencoder, Autocovariance, \n",
    "                                                  {}, \n",
    "                                                  CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_autoencoder_augmentation_crosscov = model_summary(Autoencoder, AugmentationCrossCovariances, \n",
    "                                                         {\"augmentations\": 5, \"std\": 0.15, \"knots\": 4}, \n",
    "                                                         CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_denoisingae_augmentation_crosscov = model_summary(DenoisingAutoencoder, AugmentationCrossCovariances, \n",
    "                                                         {\"augmentations\": 5, \"std\": 0.15, \"knots\": 4}, \n",
    "                                                         CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_autoencoder_ensemble_representation_crosscov = model_summary(AutoencoderEnsemble, RepresentationCrossCovariances, \n",
    "                                                                    {}, \n",
    "                                                                    CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "\n",
    "    df_grand_summary = pd.concat(\n",
    "        [\n",
    "            df_autoencoder_autocovariance,\n",
    "            df_denoisingae_autocovariance,\n",
    "            df_autoencoder_augmentation_crosscov,\n",
    "            df_denoisingae_augmentation_crosscov,\n",
    "            df_autoencoder_ensemble_representation_crosscov,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_grand_summary.sort_values(\n",
    "        by=[\n",
    "            \"Monitoring model\",\n",
    "            \"Training model\",\n",
    "            \"Training dataset\",\n",
    "            \"Testing dataset\",\n",
    "            \"Epoch\",\n",
    "            \"Split\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df_grand_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onur/Desktop/GitRepos/LatentCovarianceBasedSeismicEventDetection/kfold_environment.py:428: DtypeWarning: Columns (23,24,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  eq_metadata = pd.read_csv(eq_metadata_csv)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m grand_summary()\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mto_csv(join(EXP_NAME, OUTPUT_FILE_NAME))\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mgrand_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrand_summary\u001b[39m():\n\u001b[0;32m---> 40\u001b[0m     df_autoencoder_autocovariance \u001b[39m=\u001b[39m model_summary(Autoencoder, Autocovariance, \n\u001b[1;32m     41\u001b[0m                                                   {}, \n\u001b[1;32m     42\u001b[0m                                                   CovarianceWeightedAverage(input_param\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfcov\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     44\u001b[0m     df_denoisingae_autocovariance \u001b[39m=\u001b[39m model_summary(DenoisingAutoencoder, Autocovariance, \n\u001b[1;32m     45\u001b[0m                                                   {}, \n\u001b[1;32m     46\u001b[0m                                                   CovarianceWeightedAverage(input_param\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfcov\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     48\u001b[0m     df_autoencoder_augmentation_crosscov \u001b[39m=\u001b[39m model_summary(Autoencoder, AugmentationCrossCovariances, \n\u001b[1;32m     49\u001b[0m                                                          {\u001b[39m\"\u001b[39m\u001b[39maugmentations\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.15\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mknots\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m}, \n\u001b[1;32m     50\u001b[0m                                                          CovarianceWeightedAverage(input_param\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfcov\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mmodel_summary\u001b[0;34m(training_model_class, monitoring_model_class, method_params, metric)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m test_dataset \u001b[39min\u001b[39;00m TEST_DATASETS:\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m SPLITS:\n\u001b[0;32m---> 23\u001b[0m         roc_aucs \u001b[39m=\u001b[39m calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\n\u001b[1;32m     24\u001b[0m         val_loss \u001b[39m=\u001b[39m get_validation_losses(training_model_class, train_dataset, split)\n\u001b[1;32m     25\u001b[0m         evaluations\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mTraining dataset\u001b[39m\u001b[39m\"\u001b[39m: [train_dataset]\u001b[39m*\u001b[39mEPOCHS, \n\u001b[1;32m     26\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mTesting dataset\u001b[39m\u001b[39m\"\u001b[39m: [test_dataset]\u001b[39m*\u001b[39mEPOCHS, \n\u001b[1;32m     27\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mSplit\u001b[39m\u001b[39m\"\u001b[39m: [split]\u001b[39m*\u001b[39mEPOCHS, \n\u001b[1;32m     28\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mrange\u001b[39m(EPOCHS), \n\u001b[1;32m     29\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mValidation loss\u001b[39m\u001b[39m\"\u001b[39m: val_loss, \n\u001b[1;32m     30\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mROC-AUC\u001b[39m\u001b[39m\"\u001b[39m:roc_aucs})\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mcalculate_roc_aucs\u001b[0;34m(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_roc_aucs\u001b[39m(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric):\n\u001b[1;32m      2\u001b[0m     evaluator \u001b[39m=\u001b[39m Evaluator(EXP_NAME, training_model_class, monitoring_model_class, \n\u001b[1;32m      3\u001b[0m                           train_dataset, test_dataset, FILTERS, EPOCHS, split, method_params, metric)\n\u001b[0;32m----> 5\u001b[0m     roc_vectors \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mget_roc_vectors()\n\u001b[1;32m      6\u001b[0m     roc_aucs \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n",
      "File \u001b[0;32m~/Desktop/GitRepos/LatentCovarianceBasedSeismicEventDetection/evaluator.py:161\u001b[0m, in \u001b[0;36mEvaluator.get_roc_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_roc_vectors\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m ):\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_are_monitoring_files_present():\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtester\u001b[39m.\u001b[39mtest()\n\u001b[1;32m    164\u001b[0m     monitoring_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_monitoring_meta()\n",
      "File \u001b[0;32m~/Desktop/GitRepos/LatentCovarianceBasedSeismicEventDetection/evaluator.py:239\u001b[0m, in \u001b[0;36mEvaluator._are_monitoring_files_present\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_are_monitoring_files_present\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_are_data_files_present() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_meta_file_present()\n",
      "File \u001b[0;32m~/Desktop/GitRepos/LatentCovarianceBasedSeismicEventDetection/evaluator.py:244\u001b[0m, in \u001b[0;36mEvaluator._are_data_files_present\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_are_data_files_present\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m     data_file_existances \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs:\n\u001b[1;32m    245\u001b[0m         data_file_existances\u001b[39m.\u001b[39mappend(exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_file_path(epoch)))\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(data_file_existances)\u001b[39m.\u001b[39mall()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "df = grand_summary()\n",
    "df.to_csv(join(EXP_NAME, OUTPUT_FILE_NAME))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
